{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import collections\n",
    "import sys\n",
    "import MeCab\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#いろいろな文で使える物にしたいと考えていたがテキストの書き方が統一されていないため\n",
    "#(at_a_railway_station　loaves　chichi_kaeruなど)\n",
    "#妥協したものとなっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各語の出現回数 [('の', 105), ('こと', 103), ('私', 84), ('よう', 70), ('もの', 37), ('なか', 32), ('彼', 32), ('彼女', 27), ('的', 26), ('墓', 26), ('とき', 22), ('それ', 22), ('埋葬', 20), ('苦痛', 18), ('上', 18), ('自分', 17), ('夜', 17), ('あいだ', 16), ('病気', 16), ('一', 16)]\n"
     ]
    }
   ],
   "source": [
    "with open('hayasugiru_maiso.txt', 'r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "for line in lines:\n",
    "\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub(\"｜\", \"\",s)\n",
    "   \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    # Mecab\n",
    "    \n",
    "    node = mecab.parseToNode(s)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "        if part in [\"名詞\"]:#取得した品詞が一致するなら\n",
    "             r.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "c = collections.Counter(r)#語の出現個数をカウント、今回は\"名詞\", \"形容詞\", \"動詞\"をカウント\n",
    "print(\"各語の出現回数\",c.most_common(20))#表示 most_common()メソッドに引数nを指定すると、出現回数の多いn要素のみを返す。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#参考　https://qiita.com/sudo5in5k/items/f89d9dc1bec1ed221ede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各語の出現回数 [('し', 22), ('いる', 20), ('の', 17), ('い', 16), ('れ', 15), ('こと', 10), ('たち', 10), ('人', 10), ('よう', 9), ('犯人', 9), ('さ', 8), ('ある', 7), ('中', 7), ('男', 7), ('私', 7), ('熊本', 6), ('前', 6), ('する', 6), ('とき', 6), ('警部', 6)]\n"
     ]
    }
   ],
   "source": [
    "data= open('at_a_railway_station.txt', 'r')\n",
    "textdata=data.read()#一括で詠み込む\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　ここでは-の5回以上の繰り返しで分割している\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]#ここが「翻訳の底本」になっている\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "for line in lines:\n",
    "\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = s.replace(\"｜\", \"\")\n",
    "    #s = s.replace('\\u3000', '')#空白が半角ではなく全角の時使用\n",
    "    s = re.sub(r'《.+?》', \"\", s)# 《》を消す\n",
    "    s = re.sub(r'［.+?］', '', s)#[]\n",
    "    s = re.sub(r'（.+?）', '', s)#()\n",
    "    # Mecab\n",
    "    \n",
    "    node = mecab.parseToNode(s)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "        if part in [\"名詞\", \"形容詞\", \"動詞\"]:#取得した品詞が一致するなら\n",
    "             r.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "c =Counter(r)#語の出現個数をカウント、今回は\"名詞\", \"形容詞\", \"動詞\"をカウント\n",
    "print(\"各語の出現回数\",c.most_common(20))#表示 most_common()メソッドに引数nを指定すると、出現回数の多いn要素のみを返す。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各語の出現回数 [('僕', 17), ('し', 16), ('い', 13), ('の', 13), ('イイナ', 13), ('人', 12), ('カルメン', 11), ('君', 9), ('等', 8), ('いる', 7), ('晩', 6), ('見', 6), ('それ', 5), ('革命', 4), ('前', 4), ('ダンチェンコ', 4), ('舞台', 4), ('話し', 4), ('イイ', 4), ('云う', 4)]\n"
     ]
    }
   ],
   "source": [
    "data= open('carmen.txt', 'r')\n",
    "textdata=data.read()#一括で詠み込む\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　ここでは-の5回以上の繰り返しで分割している\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "for line in lines:\n",
    "\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = s.replace(\"｜\", \"\")\n",
    "    #s = s.replace('\\u3000', '')#空白が半角ではなく全角の時使用\n",
    "    s = re.sub(r'《.+?》', \"\", s)# 《》を消す\n",
    "    s = re.sub(r'［.+?］', '', s)#[]\n",
    "    s = re.sub(r'（.+?）', '', s)#()\n",
    "    # Mecab\n",
    "    \n",
    "    node = mecab.parseToNode(s)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "        if part in [\"名詞\", \"形容詞\", \"動詞\"]:#取得した品詞が一致するなら\n",
    "             r.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "c = collections.Counter(r)#語の出現個数をカウント、今回は\"名詞\", \"形容詞\", \"動詞\"をカウント\n",
    "print(\"各語の出現回数\",c.most_common(20))#表示 most_common()メソッドに引数nを指定すると、出現回数の多いn要素のみを返す。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #s = s.replace('\\u3000', '')#空白が半角ではなく全角の時使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000 カルメン\n",
      "芥川龍之介\n",
      "\n",
      "\n",
      "11111 \n",
      "【テキスト中に現れる記号について】\n",
      "\n",
      "《》：ルビ\n",
      "（例）露台《バルコニー》\n",
      "\n",
      "｜：ルビの付く文字列の始まりを特定する記号\n",
      "（例）革命｜前《ぜん》\n",
      "\n",
      "［＃］：入力者注　主に外字の説明や、傍点の位置の指定\n",
      "（例）カゲキ［＃「カゲキ」に傍点］派ですから\n",
      "\n",
      "22222 \n",
      "\n",
      "　革命｜前《ぜん》だったか、革命後だったか、――いや、あれは革命前ではない。なぜまた革命前ではないかと言えば、僕は当時｜小耳《こみみ》に挟《はさ》んだダンチェンコの洒落《しゃれ》を覚えているからである。\n",
      "　ある蒸し暑い雨《あま》もよいの夜《よ》、舞台監督のＴ君は、帝劇《ていげき》の露台《バルコニー》に佇《たたず》みながら、炭酸水《たんさんすい》のコップを片手に詩人のダンチェンコと話していた。あの亜麻色《あまいろ》の髪の毛をした盲目《もうもく》詩人のダンチェンコとである。\n",
      "「これもやっぱり時勢ですね。はるばる露西亜《ロシア》のグランド・オペラが日本の東京へやって来ると言うのは。」\n",
      "「それはボルシェヴィッキはカゲキ［＃「カゲキ」に傍点］派ですから。」\n",
      "　この問答のあったのは確か初日から五日《いつか》目の晩、――カルメンが舞台へ登った晩である。僕はカルメンに扮《ふん》するはずのイイナ・ブルスカアヤに夢中になっていた。イイナは目の大きい、小鼻の張った、肉感の強い女である。僕は勿論カルメンに扮《ふん》するイイナを観《み》ることを楽しみにしていた、が、第一幕が上ったのを見ると、カルメンに扮したのはイイナではない。水色の目をした、鼻の高い、何《なん》とか云う貧相《ひんそう》な女優である。僕はＴ君と同じボックスにタキシイドの胸を並べながら、落胆《らくたん》しない訣《わけ》には行かなかった。\n",
      "「カルメンは僕等のイイナじゃないね。」\n",
      "「イイナは今夜は休みだそうだ。その原因がまた頗《すこぶ》るロマンティックでね。――」\n",
      "「どうしたんだ？」\n",
      "「何《なん》とか云う旧帝国の侯爵《こうしゃく》が一人、イイナのあとを追っかけて来てね、おととい東京へ着いたんだそうだ。ところがイイナはいつのまにか亜米利加《アメリカ》人の商人の世話になっている。そいつを見た侯爵は絶望したんだね、ゆうべホテルの自分の部屋で首を縊《くく》って死んじまったんだそうだ。」\n",
      "　僕はこの話を聞いているうちに、ある場景《じょうけい》を思い出した。それは夜《よ》の更《ふ》けたホテルの一室に大勢《おおぜい》の男女《なんにょ》に囲《かこ》まれたまま、トランプを弄《もてあそ》んでいるイイナである。黒と赤との着物を着たイイナはジプシイ占《うらな》いをしていると見え、Ｔ君にほほ笑《え》みかけながら、「今度はあなたの運《うん》を見て上げましょう」と言った。（あるいは言ったのだと云うことである。ダア以外の露西亜《ロシア》語を知らない僕は勿論十二箇国の言葉に通じたＴ君に翻訳して貰うほかはない。）それからトランプをまくって見た後《のち》、「あなたはあの人よりも幸福ですよ。あなたの愛する人と結婚出来ます」と言った。あの人と云うのはイイナの側に誰かと話していた露西亜《ロシア》人である。僕は不幸にも「あの人」の顔だの服装だのを覚えていない。わずかに僕が覚えているのは胸に挿《さ》していた石竹《せきちく》だけである。イイナの愛を失ったために首を縊《くく》って死んだと云うのはあの晩の「あの人」ではなかったであろうか？……\n",
      "「それじゃ今夜は出ないはずだ。」\n",
      "「好《い》い加減に外へ出て一杯《いっぱい》やるか？」\n",
      "Ｔ君も勿論イイナ党である。\n",
      "「まあ、もう一幕見て行こうじゃないか？」\n",
      "　僕等がダンチェンコと話したりしたのは恐らくはこの幕合《まくあ》いだったのであろう。\n",
      "　次の幕も僕等には退屈だった。しかし僕等が席についてまだ五分とたたないうちに外国人が五六人ちょうど僕等の正面に当る向う側のボックスへはいって来た。しかも彼等のまっ先に立ったのは紛《まぎ》れもないイイナ・ブルスカアヤである。イイナはボックスの一番前に坐り、孔雀《くじゃく》の羽根の扇を使いながら、悠々と舞台を眺め出した。のみならず同伴の外国人の男女《なんにょ》と（その中には必ず彼女の檀那《だんな》の亜米利加人も交《まじ》っていたのであろう。）愉快そうに笑ったり話したりし出した。\n",
      "「イイナだね。」\n",
      "「うん、イイナだ。」\n",
      "　僕等はとうとう最後の幕まで、――カルメンの死骸《しがい》を擁《よう》したホセが、「カルメン！　カルメン！」と慟哭《どうこく》するまで僕等のボックスを離れなかった。それは勿論舞台よりもイイナ・ブルスカアヤを見ていたためである。この男を殺したことを何とも思っていないらしい露西亜のカルメンを見ていたためである。\n",
      "\n",
      "　　　　　　　×　　　　　　　　　　×　　　　　　　　　　×\n",
      "\n",
      "　それから二三日たったある晩、僕はあるレストランの隅にＴ君とテエブルを囲んでいた。\n",
      "「君はイイナがあの晩以来、確か左の薬指《くすりゆび》に繃帯《ほうたい》していたのに気がついているかい？」\n",
      "「そう云えば繃帯していたようだね。」\n",
      "「イイナはあの晩ホテルへ帰ると、……」\n",
      "「駄目《だめ》だよ、君、それを飲んじゃ。」\n",
      "　僕はＴ君に注意した。薄い光のさしたグラスの中にはまだ小さい黄金虫《こがねむし》が一匹、仰向《あおむ》けになってもがいていた。Ｔ君は白葡萄酒《しろぶどうしゅ》を床《ゆか》へこぼし、妙な顔をしてつけ加えた。\n",
      "「皿を壁へ叩きつけてね、そのまた欠片《かけら》をカスタネットの代りにしてね、指から血の出るのもかまわずにね、……」\n",
      "「カルメンのように踊ったのかい？」\n",
      "　そこへ僕等の興奮とは全然つり合わない顔をした、頭の白い給仕が一人、静に鮭《さけ》の皿を運んで来た。……\n",
      "［＃地から１字上げ］（大正十五年四月十日）\n",
      "\n",
      "\n",
      "\n",
      "底本：「芥川龍之介全集6」ちくま文庫、筑摩書房\n",
      "　　　1987（昭和62）年3月24日第1刷発行\n",
      "　　　1993（平成5）年2月25日第6刷発行\n",
      "底本の親本：「筑摩全集類聚版芥川龍之介全集」筑摩書房\n",
      "　　　1971（昭和46）年3月～1971（昭和46）年11月\n",
      "入力：j.utiyama\n",
      "校正：田尻幹二\n",
      "1999年1月27日公開\n",
      "2004年3月7日修正\n",
      "青空文庫作成ファイル：\n",
      "このファイルは、インターネットの図書館、青空文庫（http://www.aozora.gr.jp/）で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんです。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "textdata= open('carmen.txt', 'r').read()\n",
    "textdata2= open('carmen.txt', 'r').read()\n",
    "textdata1= open('carmen.txt', 'r').read()\n",
    "\n",
    "# 青空文庫のための固有処理\n",
    "textdata = re.split('\\-{5,}', textdata)[0]\n",
    "textdata1 = re.split('\\-{5,}', textdata1)[1]\n",
    "textdata2 = re.split('\\-{5,}', textdata2)[2]\n",
    "\n",
    "print(\"00000\",textdata)\n",
    "print(\"11111\",textdata1)\n",
    "print(\"22222\",textdata2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "　革命｜前《ぜん》だったか、革命後だったか、――いや、あれは革命前ではない。なぜまた革命前ではないかと言えば、僕は当時｜小耳《こみみ》に挟《はさ》んだダンチェンコの洒落《しゃれ》を覚えているからである。\n",
      "　ある蒸し暑い雨《あま》もよいの夜《よ》、舞台監督のＴ君は、帝劇《ていげき》の露台《バルコニー》に佇《たたず》みながら、炭酸水《たんさんすい》のコップを片手に詩人のダンチェンコと話していた。あの亜麻色《あまいろ》の髪の毛をした盲目《もうもく》詩人のダンチェンコとである。\n",
      "「これもやっぱり時勢ですね。はるばる露西亜《ロシア》のグランド・オペラが日本の東京へやって来ると言うのは。」\n",
      "「それはボルシェヴィッキはカゲキ［＃「カゲキ」に傍点］派ですから。」\n",
      "　この問答のあったのは確か初日から五日《いつか》目の晩、――カルメンが舞台へ登った晩である。僕はカルメンに扮《ふん》するはずのイイナ・ブルスカアヤに夢中になっていた。イイナは目の大きい、小鼻の張った、肉感の強い女である。僕は勿論カルメンに扮《ふん》するイイナを観《み》ることを楽しみにしていた、が、第一幕が上ったのを見ると、カルメンに扮したのはイイナではない。水色の目をした、鼻の高い、何《なん》とか云う貧相《ひんそう》な女優である。僕はＴ君と同じボックスにタキシイドの胸を並べながら、落胆《らくたん》しない訣《わけ》には行かなかった。\n",
      "「カルメンは僕等のイイナじゃないね。」\n",
      "「イイナは今夜は休みだそうだ。その原因がまた頗《すこぶ》るロマンティックでね。――」\n",
      "「どうしたんだ？」\n",
      "「何《なん》とか云う旧帝国の侯爵《こうしゃく》が一人、イイナのあとを追っかけて来てね、おととい東京へ着いたんだそうだ。ところがイイナはいつのまにか亜米利加《アメリカ》人の商人の世話になっている。そいつを見た侯爵は絶望したんだね、ゆうべホテルの自分の部屋で首を縊《くく》って死んじまったんだそうだ。」\n",
      "　僕はこの話を聞いているうちに、ある場景《じょうけい》を思い出した。それは夜《よ》の更《ふ》けたホテルの一室に大勢《おおぜい》の男女《なんにょ》に囲《かこ》まれたまま、トランプを弄《もてあそ》んでいるイイナである。黒と赤との着物を着たイイナはジプシイ占《うらな》いをしていると見え、Ｔ君にほほ笑《え》みかけながら、「今度はあなたの運《うん》を見て上げましょう」と言った。（あるいは言ったのだと云うことである。ダア以外の露西亜《ロシア》語を知らない僕は勿論十二箇国の言葉に通じたＴ君に翻訳して貰うほかはない。）それからトランプをまくって見た後《のち》、「あなたはあの人よりも幸福ですよ。あなたの愛する人と結婚出来ます」と言った。あの人と云うのはイイナの側に誰かと話していた露西亜《ロシア》人である。僕は不幸にも「あの人」の顔だの服装だのを覚えていない。わずかに僕が覚えているのは胸に挿《さ》していた石竹《せきちく》だけである。イイナの愛を失ったために首を縊《くく》って死んだと云うのはあの晩の「あの人」ではなかったであろうか？……\n",
      "「それじゃ今夜は出ないはずだ。」\n",
      "「好《い》い加減に外へ出て一杯《いっぱい》やるか？」\n",
      "Ｔ君も勿論イイナ党である。\n",
      "「まあ、もう一幕見て行こうじゃないか？」\n",
      "　僕等がダンチェンコと話したりしたのは恐らくはこの幕合《まくあ》いだったのであろう。\n",
      "　次の幕も僕等には退屈だった。しかし僕等が席についてまだ五分とたたないうちに外国人が五六人ちょうど僕等の正面に当る向う側のボックスへはいって来た。しかも彼等のまっ先に立ったのは紛《まぎ》れもないイイナ・ブルスカアヤである。イイナはボックスの一番前に坐り、孔雀《くじゃく》の羽根の扇を使いながら、悠々と舞台を眺め出した。のみならず同伴の外国人の男女《なんにょ》と（その中には必ず彼女の檀那《だんな》の亜米利加人も交《まじ》っていたのであろう。）愉快そうに笑ったり話したりし出した。\n",
      "「イイナだね。」\n",
      "「うん、イイナだ。」\n",
      "　僕等はとうとう最後の幕まで、――カルメンの死骸《しがい》を擁《よう》したホセが、「カルメン！　カルメン！」と慟哭《どうこく》するまで僕等のボックスを離れなかった。それは勿論舞台よりもイイナ・ブルスカアヤを見ていたためである。この男を殺したことを何とも思っていないらしい露西亜のカルメンを見ていたためである。\n",
      "\n",
      "　　　　　　　×　　　　　　　　　　×　　　　　　　　　　×\n",
      "\n",
      "　それから二三日たったある晩、僕はあるレストランの隅にＴ君とテエブルを囲んでいた。\n",
      "「君はイイナがあの晩以来、確か左の薬指《くすりゆび》に繃帯《ほうたい》していたのに気がついているかい？」\n",
      "「そう云えば繃帯していたようだね。」\n",
      "「イイナはあの晩ホテルへ帰ると、……」\n",
      "「駄目《だめ》だよ、君、それを飲んじゃ。」\n",
      "　僕はＴ君に注意した。薄い光のさしたグラスの中にはまだ小さい黄金虫《こがねむし》が一匹、仰向《あおむ》けになってもがいていた。Ｔ君は白葡萄酒《しろぶどうしゅ》を床《ゆか》へこぼし、妙な顔をしてつけ加えた。\n",
      "「皿を壁へ叩きつけてね、そのまた欠片《かけら》をカスタネットの代りにしてね、指から血の出るのもかまわずにね、……」\n",
      "「カルメンのように踊ったのかい？」\n",
      "　そこへ僕等の興奮とは全然つり合わない顔をした、頭の白い給仕が一人、静に鮭《さけ》の皿を運んで来た。……\n",
      "［＃地から１字上げ］（大正十五年四月十日）\n",
      "\n",
      "\n",
      "\n",
      "底本：「芥川龍之介全集6」ちくま文庫、筑摩書房\n",
      "　　　1987（昭和62）年3月24日第1刷発行\n",
      "　　　1993（平成5）年2月25日第6刷発行\n",
      "底本の親本：「筑摩全集類聚版芥川龍之介全集」筑摩書房\n",
      "　　　1971（昭和46）年3月～1971（昭和46）年11月\n",
      "入力：j.utiyama\n",
      "校正：田尻幹二\n",
      "1999年1月27日公開\n",
      "2004年3月7日修正\n",
      "青空文庫作成ファイル：\n",
      "このファイルは、インターネットの図書館、青空文庫（http://www.aozora.gr.jp/）で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんです。\n",
      "\n",
      "各単語の出現回数 [('いる', 21), ('僕', 18), ('する', 18), ('の', 15), ('イイナ', 13), ('人', 13), ('君', 10), ('カルメン', 10), ('等', 8), ('それ', 7), ('見る', 7), ('晩', 6), ('云う', 6), ('言う', 5), ('革命', 4), ('前', 4), ('ダンチェンコ', 4), ('舞台', 4), ('話す', 4), ('露西亜', 4)]\n",
      "単語の異なり数 272\n",
      "総数 509\n"
     ]
    }
   ],
   "source": [
    "\n",
    "textdata= open('carmen.txt', 'r').read()\n",
    "\n",
    "# 青空文庫のための固有処理\n",
    "textdata = re.split('\\-{5,}', textdata)[2]\n",
    "print(textdata)\n",
    "textdata = re.split('底本：', textdata)[0]\n",
    "textdata = textdata.strip()\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "results = []\n",
    "lines = textdata.split(\"\\r\\n\")\n",
    "for line in lines:\n",
    "    r = []\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = s.replace(\"｜\", \"\")\n",
    "    s = s.replace('\\u3000', '')\n",
    "    s = re.sub(r'《.+?》', \"\", s)\n",
    "    s = re.sub(r'［.+?］', '', s)\n",
    "    # Mecab\n",
    "    node = mecab.parseToNode(s)\n",
    "    while node:\n",
    "        # 単語を取得\n",
    "        if node.feature.split(\",\")[6] == '*':\n",
    "            word = node.surface\n",
    "        else:\n",
    "            word = node.feature.split(\",\")[6]\n",
    "\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]\n",
    "\n",
    "        if part in [\"名詞\", \"形容詞\", \"動詞\"]:#, \"記号\"\n",
    "            r.append(word)\n",
    "            \n",
    "        node = node.next\n",
    "    rl = (\" \".join(r)).strip()\n",
    "    results.append(rl)\n",
    "\n",
    "#print(results)\n",
    "c = collections.Counter(r)#単語の出現個数をカウント\n",
    "print(\"各単語の出現回数\",c.most_common(20))#表示 most_common()メソッドに引数nを指定すると、出現回数の多いn要素のみを返す。\n",
    "print(\"単語の異なり数\",len(c))#重複しない要素（一意な要素）の個数（種類）をカウント\n",
    "print(\"総数\",len(r))#リストのサイズの取得(len関数)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['カルメン', '芥川', '龍之介', '革命', '前', '革命', '後', 'あれ', '革命', '前', '革命', '前', '言え', '僕', '当時', '小耳', '挟ん', 'ダンチェンコ', '洒落', '覚え', 'いる', '蒸し暑い', '雨', 'よい', '夜', '舞台', '監督', '君', '帝劇', '露台', '佇み', '炭酸', '水', 'コップ', '片手', '詩人', 'ダンチェンコ', '話し', 'い', '亜麻', '色', '髪の毛', 'し', '盲目', '詩人', 'ダンチェンコ', 'これ', '時勢', '露西亜', 'グランド', 'オペラ', '日本', '東京', 'やって来る', '言う', 'の', 'それ', 'ボルシェヴィッキ', 'カゲキ', '派', '問答', 'あっ', 'の', '確か', '初日', '五', '日', '目', '晩', 'カルメン', '舞台', '登っ', '晩', '僕', 'カルメン', '扮する', 'はず', 'イイナ・ブルスカアヤ', '夢中', 'なっ', 'い', 'イイナ', '目', '大きい', '小鼻', '張っ', '肉感', '強い', '女', '僕', 'カルメン', '扮する', 'イイナ', '観る', 'こと', '楽しみ', 'し', 'い', '一幕', '上っ', 'の', '見る', 'カルメン', '扮', 'し', 'の', 'イイ', '水色', '目', 'し', '鼻', '高い', '云う', '貧相', '女優', '僕', '君', 'ボックス', 'タキシイド', '胸', '並べ', '落胆', 'し', '訣', '行か', 'カルメン', '僕', '等', 'イイナ', 'イイナ', '今夜', '休み', 'そう', '原因', 'ロマンティック', 'し', 'ん', '云う', '帝国', '侯爵', '一', '人', 'イイナ', 'あと', '追っかけ', '来', 'おととい', '東京', '着い', 'ん', 'そう', 'イイナ', '亜米利加', '人', '商人', '世話', 'なっ', 'いる', 'そいつ', '見', '侯爵', '絶望', 'し', 'ん', 'ゆうべ', 'ホテル', '自分', '部屋', '首', '縊', '死ん', 'じまっ', 'ん', 'そう', '僕', '話', '聞い', 'いる', 'うち', '場景', '思い出し', 'それ', '夜', '更け', 'ホテル', '一室', '大勢', '男女', '囲ま', 'れ', 'まま', 'トランプ', '弄ん', 'いる', 'イイナ', '黒', '赤', '着物', '着', 'イイ', 'ジプシイ', '占い', 'し', 'いる', '見え', '君', 'ほほ笑みかけ', '今度', 'あなた', '運', '見', '上げ', '言っ', '言っ', 'の', '云う', 'こと', 'ダア', '以外', '露西亜', '語', '知ら', '僕', '十', '二', '箇国', '言葉', '通じ', '君', '翻訳', 'し', '貰う', 'ほか', 'それ', 'トランプ', 'まくっ', '見', '後', 'あなた', '人', '幸福', 'あなた', '愛する', '人', '結婚', '出来', '言っ', '人', '云う', 'の', 'イイ', '側', '誰', '話し', 'い', '露西亜', '人', '僕', '不幸', '人', '顔', '服装', 'の', '覚え', 'い', '僕', '覚え', 'いる', 'の', '胸', '挿し', 'い', '石竹', 'イイナ', '愛', '失っ', 'ため', '首', '縊', '死ん', '云う', 'の', '晩', '人', 'なかっ', 'それ', '今夜', '出', 'はず', '好い加減', '外', '出', 'やる', '君', 'イイ', '党', '一幕', '見', '行こ', '僕', '等', 'ダンチェンコ', '話し', 'し', 'の', '幕', '合い', 'の', '次', '幕', '僕', '等', '退屈', '僕', '等', '席', '五', '分', 'たた', 'うち', '外国', '人', '五', '六', '人', '僕', '等', '正面', '当る', '向う側', 'ボックス', 'はいっ', '来', '彼等', '先', '立っ', 'の', '紛れ', 'ない', 'イイナ・ブルスカアヤ', 'イイナ', 'ボックス', '一番', '前', '坐り', '孔雀', '羽根', '扇', '使い', '舞台', '眺め', '出し', 'なら', '同伴', '外国', '人', '男女', '中', '彼女', '檀那', '亜米利加', '人', '交っ', 'い', 'の', '愉快', 'そう', '笑っ', '話し', 'し', '出し', 'イイナ', 'イイナ', '僕', '等', '最後', '幕', 'カルメン', '死骸', '擁し', 'ホセ', 'カルメン', 'カルメン', '慟哭', 'する', '僕', '等', 'ボックス', '離れ', 'それ', '舞台', 'イイナ・ブルスカアヤ', '見', 'い', 'ため', '男', '殺し', 'こと', '思っ', 'い', '露西亜', 'カルメン', '見', 'い', 'ため', 'それ', '二', '三', '日', 'たっ', '晩', '僕', 'ある', 'レストラン', '隅', '君', 'テエブル', '囲ん', 'い', '君', 'イイナ', '晩', '以来', '確か', '左', '薬指', '繃帯', 'し', 'い', '気', 'つい', 'いる', '云え', '繃帯', 'し', 'い', 'よう', 'イイナ', '晩', 'ホテル', '帰る', '駄目', '君', 'それ', '飲ん', '僕', '君', '注意', 'し', '薄い', '光', 'さし', 'グラス', '中', '小さい', '黄金虫', '一', '匹', '仰向け', 'なっ', 'もがい', 'い', '君', '白', '葡萄', '酒', '床', 'こぼし', '妙', '顔', 'し', 'つけ加え', '皿', '壁', '叩きつけ', '欠片', 'カスタネット', '代り', 'し', '指', '血', '出る', 'の', 'かまわ', 'カルメン', 'よう', '踊っ', 'の', 'そこ', '僕', '等', '興奮', 'つり合わ', '顔', 'し', '頭', '白い', '給仕', '一', '人', '静', '鮭', '皿', '運ん', '来', '大正', '十', '五', '年', '四月', '十', '日']\n",
      "各単語の出現回数 [('僕', 18), ('し', 17), ('の', 15), ('い', 14), ('イイナ', 13), ('人', 13), ('カルメン', 11), ('君', 10), ('等', 8), ('いる', 7), ('それ', 7), ('晩', 6), ('見', 6), ('云う', 5), ('革命', 4), ('前', 4), ('ダンチェンコ', 4), ('舞台', 4), ('話し', 4), ('露西亜', 4)]\n",
      "単語の異なり数 287\n",
      "総数 512\n"
     ]
    }
   ],
   "source": [
    "data= open('carmen.txt', 'r')\n",
    "textdata=data.read()\n",
    "textdata = re.split('\\-{5,}', textdata)[0]+ re.split('\\-{5,}', textdata)[2]\n",
    "textdata = re.split('底本：', textdata)[0]\n",
    "textdata = textdata.strip()#stripメソッドで改行コードを取り除く\n",
    "\n",
    "#print(textdata)\n",
    "mecab = MeCab.Tagger()\n",
    "results = []\n",
    "lines = textdata.split(\"\\r\\n\")\n",
    "for line in lines:\n",
    "    r = []\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = s.replace(\"｜\", \"\")\n",
    "    s = s.replace('\\u3000', '')\n",
    "    s = re.sub(r'《.+?》', \"\", s)\n",
    "    s = re.sub(r'［.+?］', '', s)\n",
    "    # Mecab\n",
    "    node = mecab.parseToNode(s)#parseToNode()を使うと形態素の詳細情報\n",
    "    while node:\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報\n",
    "\n",
    "        if part in [\"名詞\", \"形容詞\", \"動詞\"]:#, \"記号\"\n",
    "             r.append(word)#words.split()\n",
    "            \n",
    "        node = node.next\n",
    "    \n",
    "#print(r)\n",
    "c = collections.Counter(r)#単語の出現個数をカウント\n",
    "print(\"各単語の出現回数\",c.most_common(20))#表示 most_common()メソッドに引数nを指定すると、出現回数の多いn要素のみを返す。\n",
    "print(\"単語の異なり数\",len(c))#重複しない要素（一意な要素）の個数（種類）をカウント\n",
    "print(\"総数\",len(r))#リストのサイズの取得(len関数)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "停車場にて\n",
      "AT A RAILWAY STATION\n",
      "小泉八雲　Lafcadio Hearn\n",
      "林田清明訳\n",
      "\n",
      "\n",
      "\n",
      "［＃地付き］明治二六年六月七日\n",
      "\n",
      "　きのうの福岡発信の電報によると、当地で逮捕された兇徒が、裁判のために、きょう正午着の汽車で熊本へ護送されるということだった。熊本の警察官が、この兇徒を引取るために福岡に出張していたのである。\n",
      "　四年前、熊本市｜相撲町《すもうちょう》のある家に、夜半、盗人が押し入り、家人らを脅して、縛り上げ、高価な財産を盗んだ。警察がうまく追跡して、盗人は二四時間以内に逮捕されたので盗品を処分することもできなかった。ところが、警察署に連行されるとき、捕縄《とりなわ》を解《ほど》き、サーベルを奪い、巡査を殺害して逃走したのである。つい先週までこの兇徒の行方はまるっきりつかめなかった。\n",
      "　ところが、たまたま福岡の監獄所を訪れていた熊本の刑事が、四年もの間、写真のように脳裏に焼き付けていた顔を、囚人たちの中に見つけたのである。\n",
      "　「あの男は？」獄吏に尋ねた。\n",
      "　「窃盗犯でありますが、ここでは草部と記録されております。」\n",
      "　刑事は囚人のところに歩み寄ると、言った、\n",
      "　「お前の名前は草部ではないな。熊本の殺人容疑でお尋ね者の、野村禎一だ。」\n",
      "　重罪犯人はすっかり白状したのである。\n",
      "\n",
      "　停車場に到着するのを見届けようと私も出かけたが、かなりの人が詰めかけている。人々が憤るのをたぶん見聞きするだろうと思っていたし、一悶着《ひともんちゃく》起こりはしないかとすら恐れてもいた。殺された巡査は周囲《まわり》からとても好かれていたし、彼の身内の者も、おそらくこの見物人たちの中にいるはずである。熊本の群集もとても温和《おとな》しいとはいえないのである。それで警備のために多数の警官が配置されているものとばかり思っていたが、私の予想は外れた。\n",
      "　汽車は、下駄を履いた乗客たちのあわてた急ぎ足やカラコロという音が響き、また新聞やラムネなど飲み物を売る少年たちの呼び声などで、いつものようにあわただしく、また騒々しい光景の中に停車した。改札口の外で、私たちは五分近くも待っていた。そのとき、警部が改札口の扉を押し開けて出てきて、犯人が現れる――大柄の粗野な感じの男で、顔は俯《うつむ》き加減にしており、両の手は背中で縛られている。犯人と護送の巡査は二人とも改札口の前で、立ち止まった。そして、詰めかけている人たちが黙って一目見ようと前の方に押し寄せた。そのとき、警部が叫んだ。\n",
      "　「杉原さん！　杉原おきびさん！　いませんか？」\n",
      "　「はい！」と声がすると、私の近くに立っていた、子どもを背負った細身の小柄な婦人が人混みをかき分けて進み出た。この人は殺された巡査の妻で、背負っているのが息子である。警部が手を前後に振るしぐさをすると、群衆は後ろずさりに下がった。そうして、犯人と護衛の警官のためのスペースが出来た。この空間で子どもを背負った未亡人と殺人者とが向き合って立つことになった。あたりは静まり返っている。\n",
      "　そして、警部がこの未亡人にではなく、子どもに話しかけた。低い声だが、はっきりと喋ったので、一言一言が明瞭に聞き取れた。\n",
      "　「坊や、この男が四年前にあんたのお父《とつ》さんを殺したんだよ。あんたはまだ生まれちゃいなくて、お母《つか》さんのお腹の中にいたんだからなぁ。あんたを可愛がってくれるはずのお父《とつ》さんがいないのは、この男の仕業だよ。見てご覧――ここで警部は犯人の顎に手をやり、しっかりと彼の目を向けるようにした――坊や、よく見てご覧、こいつを！　怖がらなくていいから。辛いだろうが、そうしなくちゃいけない。あの男を見るんだ！」\n",
      "　母親の肩越しに、坊やは怖がってでもいるかのように、眼を見開いて見つめる。そして、今度はしゃくり泣き始め、涙が溢れてくる。坊やは、しっかりと、また言われたように男をじっと見つめている。まっすぐにその卑屈な顔をずっと覗き込んでいた。\n",
      "　周りの人たちも息を呑んだようである。\n",
      "　犯人の表情がゆがむのが見えた。後ろ手に縛られているにもかかわらず、彼は膝の上に崩れ落ち、顔を土埃《ほこり》の中に打ちつけて、人の心を震わせるような、しゃがれた声で自責の念に駆られて、しばらく嗚咽《おえつ》していた。\n",
      "　「済まない！　許してくれ！　坊や、堪忍しておくれ！　憎んでいたからじゃねぇんだ。怖かったばかりに、ただ逃げようと思ってやっちまったんだ。俺がなにもかも悪いんだ。あんたに、まったく取り返しの付かない、悪いことをしちまった！　罪を償わなくちゃならねぇ。死にてぇだ。そう喜んで死にますとも！　ですから、坊や、お情けと思って、俺を許しておくんなせぇ！」\n",
      "　男の子は静かにまだしゃくり泣いている。警部は肩を震わせている犯人の男を引き起こした。黙りこくったままだった人々は、左右に分かれて道を空《あ》けた。するとそのとき、まったく突然に、群衆がみなすすり泣き始めたのである。銅像のような表情をした護送の警官がそばを通りすぎるとき、私は以前にも見たことのないもの――ほとんどの人もかつて見たことのない――そして私もおそらく再び見ることのないであろう――日本の警官の涙を目撃したのである。\n",
      "\n",
      "　人だかりも潮《しお》が引くように少なくなった。私は取り残され、この場の不思議な教訓について考えている。ここには、自分が犯した犯罪行為のために遺児となり、未亡人となったという明白な結末を目の当たりにして、心情的に犯罪の意味について悟らせるという、本来そうあるべきだが、温情ある裁きがあったのである。ここには、死を前にしてひたすら赦《ゆる》しを乞う、一途な後悔の念があった。また、ここには、怒りだせばこの国の中では最も危険な庶民がいた――ところが、この人たちは、人生の困難さや人間の弱さを純朴に、また身にしみて経験しているので、激しい怒りではなく、罪についての大きな悲しみだけで胸塞がれ、後悔の念と恥を知ることで満足しており、またあらゆることに感動し、何もかもを分かっているのであった。\n",
      "\n",
      "　このエピソードのもっとも重要な事実は、それがきわめて東洋的であるからだが、つぎのことにある。犯人を悔い改めさせたのは、彼自身も持っている、子に対する父親の心情に訴えたからであった――子どもたちへの深い愛情こそが、あらゆる日本人の心の大きな部分を占めているのである。\n",
      "\n",
      "　日本では最もよく知られた盗賊の石川五右衛門に、つぎの話がある。ある夜、殺して、盗みを働こうと人家に忍び込んだときに、自分に両手を差し伸べている赤ん坊の微笑みに、五右衛門はすっかり気を奪われた。そして、この無邪気な幼子と遊んでいるうちに、自分の所期の目的を達成する機会を失ったというのである。\n",
      "　これは信じられない話ではない。警察の記録には、毎年、プロの犯罪人たちが子どもらに示した同情の報告がある。地方新聞に載った、数ヶ月前の凄惨な大量殺人事件は、強盗が睡眠中の一家七人を文字通りに切り刻んだものであった。警察は、一面の血の海の中でひとり泣いている小さな男の子を発見したが、まったくの無傷であった。警察によれば、犯人らが子どもを傷つけまいとしてかなり用心した確かな証拠があるという。\n",
      "\n",
      "\n",
      "\n",
      "翻訳の\n",
      "各単語の出現回数 [('し', 22), ('いる', 20), ('の', 17), ('い', 16), ('れ', 15), ('こと', 10), ('たち', 10), ('人', 10), ('よう', 9), ('犯人', 9), ('さ', 8), ('ある', 7), ('中', 7), ('男', 7), ('私', 7), ('熊本', 6), ('前', 6), ('する', 6), ('とき', 6), ('警部', 6)]\n",
      "単語の異なり数 477\n",
      "総数 803\n"
     ]
    }
   ],
   "source": [
    "data= open('at_a_railway_station.txt', 'r')\n",
    "textdata=data.read()\n",
    "textdata =re.split('\\-{5,}', textdata)[0]+ re.split('\\-{5,}', textdata)[2]\n",
    "textdata = re.split('底本：', textdata)[0]\n",
    "textdata = textdata.strip()#stripメソッドで改行コードを取り除く\n",
    "\n",
    "#print(textdata)\n",
    "mecab = MeCab.Tagger()\n",
    "results = []\n",
    "lines = textdata.split(\"\\r\\n\")\n",
    "for line in lines:\n",
    "    r = []\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    print(s)\n",
    "    s = s.replace(\"｜\", \"\")\n",
    "    s = s.replace('\\u3000', '')\n",
    "    s = re.sub(r'《.+?》', \"\", s)\n",
    "    s = re.sub(r'［.+?］', '', s)\n",
    "    \n",
    "    # Mecab\n",
    "    node = mecab.parseToNode(s)#parseToNode()を使うと形態素の詳細情報\n",
    "    while node:\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報\n",
    "\n",
    "        if part in [\"名詞\", \"形容詞\", \"動詞\"]:#, \"記号\"\n",
    "             r.append(word)#words.split()\n",
    "            \n",
    "        node = node.next\n",
    "    \n",
    "#print(r)\n",
    "c = collections.Counter(r)#単語の出現個数をカウント\n",
    "print(\"各単語の出現回数\",c.most_common(20))#表示 most_common()メソッドに引数nを指定すると、出現回数の多いn要素のみを返す。\n",
    "print(\"単語の異なり数\",len(c))#重複しない要素（一意な要素）の個数（種類）をカウント\n",
    "print(\"総数\",len(r))#リストのサイズの取得(len関数)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(r)\n",
    "#print(lines)\n",
    "#textdata = textdata.strip()#stripメソッドで改行コードを取り除く\n",
    "#print(textdata)\n",
    "    #print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"単語の異なり数\",len(c))#重複しない要素（一意な要素）の個数（種類）をカウント\n",
    "print(\"総数\",len(r))#リストのサイズの取得(len関数) \n",
    "#bindata = open('carmen.txt', 'r').read()#,encoding=\"utf-8_sig\"\n",
    "#textdata = bindata.decode('shift_jis')\n",
    "\n",
    "#\"Shift-JIS\", \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write to a file\n",
    "w_file = \"ningen_result.txt\"\n",
    "with open(w_file, 'w', encoding='utf-8') as wf:\n",
    "    wf.write(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
