{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import collections\n",
    "import sys\n",
    "import MeCab\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#５つの文章のtf-idf を求め，各文書を特徴付ける単語を20件程度求める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['私' 'の' 'こと' 'よう' 'もの' 'なか' '彼' '墓' '的' 'とき' '右' '行' '小書き' '埋葬' '彼女' '苦痛'\n",
      "  '上' 'あいだ' '病気' 'それ']\n",
      " ['僕' 'イイナ' 'カルメン' 'の' '君' '人' '等' '晩' 'それ' 'ダンチェンコ' '革命' '露西亜' 'ボックス' '舞台'\n",
      "  'い' 'ん' 'ロシア' '扮' 'イイナ・ブルスカアヤ' '幕']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "mecab = MeCab.Tagger()    # hayasugiru_maiso\n",
    "def tokenize(text):\n",
    "    \n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "        if part in [\"名詞\"]:#取得した品詞が一致するなら\n",
    "             token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "\n",
    "\n",
    "with open('hayasugiru_maiso.txt', 'r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "\n",
    "\n",
    "with open('carmen.txt', 'r')as f1:\n",
    "    textdata1=f1.read()#一括で詠み込む インデント\n",
    "textdata1 = re.split('-{5,}', textdata1)[0]+ re.split('-{5,}', textdata1)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata1 = re.split('底本', textdata1)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata1 = re.split('［＃ここから', textdata1)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines1 = textdata1.split(\"\\r\\n\")#改行で分ける\n",
    "r1 = []\n",
    "n=0\n",
    "for line1 in lines1:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s1 = line1\n",
    "    s1 = re.sub('｜', '',s)\n",
    "    s1 = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s1 = re.sub('［.+?］', '', s)#[]\n",
    "    s1 = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r1.append(s1)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  #,r2,r3,r4\n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "corpus=[textdata,textdata1]  \n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "\n",
    "#numpy の配列 ndarray のソーティングより\n",
    "\n",
    "#vectorizer.get_feature_names() 特徴名（単語）\n",
    "#tfidf_matrix.toarray()  # tf-idfの行列\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "#print(population)\n",
    "indices = np.argsort(-population)\n",
    "#品詞を名詞に限定した場合\n",
    "#大きい値ほど各文書を特徴付ける単語　 値の多い順に20件表示\n",
    "print(name_list[indices[::,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('akai_heya.txt', 'r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r3 = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)    \n",
    "    \n",
    "    \n",
    "\n",
    "with open('meijinden.txt', 'r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r4 = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tfidf_matrix.toarray())   # TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['宿' '口実' '古く' '叩い' '真夜中' '叫ぼ' '叫ん' '台' '号' '吉' '真に' '同様' '名' '名前' '口'\n",
      "  '名望' '否定' '吸わ' '告げる' '真っ黒']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "mecab = MeCab.Tagger()    # hayasugiru_maiso\n",
    "def tokenize(text):\n",
    "    #token_list = mecab.parse(text).split()\n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "\n",
    "\n",
    "with open('hayasugiru_maiso.txt', 'r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)\n",
    "#限定しなかった場合\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['あと' '指' '日本' '時勢' '最後' '服装' '楽しみ' '欠片' '扮' '次' '死骸' '気' '水' '水色' '注意'\n",
      "  '洒落' '派' '正面' '炭酸' '扇']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "mecab = MeCab.Tagger()    # carmen\n",
    "def tokenize(text):\n",
    "    #token_list = mecab.parse(text).split()\n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "        if part in [\"名詞\"]:#取得した品詞が一致するなら\n",
    "             token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "with open('carmen.txt','r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('\\u3000', '',s)\n",
    "    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)\n",
    "#品詞を名詞に限定した場合\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['分' '仰向け' '休み' '佇み' '登っ' '何とも' '使い' '側' '男' '先' '光' '党' '全然' '六' '片手'\n",
      "  '炭酸' '出る' '出来' '派' '初日']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "mecab = MeCab.Tagger()    # carmen\n",
    "def tokenize(text):\n",
    "    #token_list = mecab.parse(text).split()\n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "with open('carmen.txt','r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('\\u3000', '',s)\n",
    "    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)\n",
    "#限定しなかった場合\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Obscene' '条' '東京' '横手' '歌舞' '歓喜' '止め' '此人' '歴史' '死に場所' '死人' '杖' '死傷'\n",
      "  '殆ど' '母親' '毒' '比べもの' '気の毒' '気味' '気絶']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "mecab = MeCab.Tagger()    # akai_heya\n",
    "def tokenize(text):\n",
    "    #token_list = mecab.parse(text).split()\n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "        if part in [\"名詞\"]:#取得した品詞が一致するなら\n",
    "             token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "with open('akai_heya.txt','r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('\\u3000', '',s)\n",
    "    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)\n",
    "#品詞を名詞に限定した場合\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['射た' '名前' '示し' '同様' '同時' '程なく' '合わさる' '合わ' '確か' '合っ' '合う' '程遠く' '叱ら' '台'\n",
      "  '可哀相' '可也' '種明し' '合せる' '叫び' '破っ']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "mecab = MeCab.Tagger()    # akai_heya\n",
    "def tokenize(text):\n",
    "    #token_list = mecab.parse(text).split()\n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "with open('akai_heya.txt','r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('\\u3000', '',s)\n",
    "    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)\n",
    "#限定しなかった場合\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['いさかい' '狼' '独り' '狂い' '牽挺' '物色' '物分り' '爺さん' '燕' '熟睡' '煙' '煕' '無理' '狼狽'\n",
      "  '無為' '無形' '無双' '為' '灰色' '灰神楽']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "mecab = MeCab.Tagger()    # meijinden\n",
    "def tokenize(text):\n",
    "    #token_list = mecab.parse(text).split()\n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "        if part in [\"名詞\"]:#取得した品詞が一致するなら\n",
    "             token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "with open('meijinden.txt','r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('\\u3000', '',s)\n",
    "    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)\n",
    "#品詞を名詞に限定した場合\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['寄る' '掉尾' '掛け' '掠め' '探し出し' '揚げ' '換え' '揺らい' '携え' '擁し' '放っ' '故に' '敦' '敵'\n",
      "  '敵す' '文字通り' '断ち' '断れ' '断崖' '斯道']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "mecab = MeCab.Tagger()    # meijinden\n",
    "def tokenize(text):\n",
    "    #token_list = mecab.parse(text).split()\n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "with open('meijinden.txt','r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('\\u3000', '',s)\n",
    "    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)\n",
    "#限定しなかった場合\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A' '明瞭' '時間' '本来' '東洋' '林田' '機会' '正午' '死' '殺害' '母親' '毎年' '気' '海' '清明'\n",
      "  '温和' '温情' '満足' '潮' '無傷']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "mecab = MeCab.Tagger()    # at_a_railway_station\n",
    "def tokenize(text):\n",
    "    #token_list = mecab.parse(text).split()\n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "        if part in [\"名詞\"]:#取得した品詞が一致するなら\n",
    "             token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "with open('at_a_railway_station.txt','r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('\\u3000', '',s)\n",
    "    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)\n",
    "#品詞を名詞に限定した場合\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['売る' '夜半' '夜' '多数' '外れ' '外' '粗野' '純朴' '塞が' '場' '報告' '立つ' '堪忍' '地方' '土埃'\n",
      "  '国' '困難' '経験' '結末' '嗚咽']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "mecab = MeCab.Tagger()    # at_a_railway_station\n",
    "def tokenize(text):\n",
    "    #token_list = mecab.parse(text).split()\n",
    "    token_list=[]\n",
    "    node = mecab.parseToNode(text)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        token_list.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "with open('at_a_railway_station.txt','r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('\\u3000', '',s)\n",
    "    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)\n",
    "#限定しなかった場合\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Allan' '歳月' '歴史' '死人' '気の毒' '気付け' '気違い' '水' '氷' '永遠' '求婚' '沈黙' '油' '法廷'\n",
      "  '法律' '洞窟' '活溌' '流' '歯車' '流れ']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "# hayasugiru_maiso\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['寂滅' '古く' '叩い' '真実' '叫ぼ' '叫ん' '台' '号' '吉' '真夜中' '同様' '名' '名前' '名望' '口実'\n",
      "  '君臨' '吸わ' '告げる' '真に' '周知']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "# hayasugiru_maiso\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['あと' '指' '日本' '時勢' '最後' '服装' '楽しみ' '欠片' '扮' '次' '死骸' '気' '水' '水色' '注意'\n",
      "  '洒落' '派' '正面' '炭酸' '扇']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "# carmen \n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['分' '仰向け' '休み' '佇み' '登っ' '何とも' '使い' '側' '男' '先' '光' '党' '全然' '六' '片手'\n",
      "  '炭酸' '出る' '出来' '派' '初日']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "# carmen \n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Obscene' '条' '東京' '横手' '歌舞' '歓喜' '止め' '此人' '歴史' '死に場所' '死人' '杖' '死傷'\n",
      "  '殆ど' '母親' '毒' '比べもの' '気の毒' '気味' '気絶']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "# akai_heya\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['射た' '名前' '示し' '同様' '同時' '程なく' '合わさる' '合わ' '確か' '合っ' '合う' '程遠く' '叱ら' '台'\n",
      "  '可哀相' '可也' '種明し' '合せる' '叫び' '破っ']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "# akai_heya\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['いさかい' '狼' '独り' '狂い' '牽挺' '物色' '物分り' '爺さん' '燕' '熟睡' '煙' '煕' '無理' '狼狽'\n",
      "  '無為' '無形' '無双' '為' '灰色' '灰神楽']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "#meijinden \n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['寄る' '掉尾' '掛け' '掠め' '探し出し' '揚げ' '換え' '揺らい' '携え' '擁し' '放っ' '故に' '敦' '敵'\n",
      "  '敵す' '文字通り' '断ち' '断れ' '断崖' '斯道']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "# meijinden \n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A' '明瞭' '時間' '本来' '東洋' '林田' '機会' '正午' '死' '殺害' '母親' '毎年' '気' '海' '清明'\n",
      "  '温和' '温情' '満足' '潮' '無傷']]\n"
     ]
    }
   ],
   "source": [
    "#品詞を名詞に限定した場合\n",
    "# at_a_railway_station\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['売る' '夜半' '夜' '多数' '外れ' '外' '粗野' '純朴' '塞が' '場' '報告' '立つ' '堪忍' '地方' '土埃'\n",
      "  '国' '困難' '経験' '結末' '嗚咽']]\n"
     ]
    }
   ],
   "source": [
    "#限定しなかった場合\n",
    "# at_a_railway_station\n",
    "print(name_list[indices[::-1,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(r)\n",
    "print(vectorizer.get_feature_names()) \n",
    "print(matrix.toarray())\n",
    "print(name_list[indices[::-1]])\n",
    "print(tfidf_matrix.toarray())# TF-IDF\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "indices = np.argsort(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x\n",
    "tagger = MeCab.Tagger('-Owakati')    # 品詞情報が不要の場合\n",
    "def tokenize(text):\n",
    "    token_list = tagger.parse(text).split()\n",
    "    \n",
    "    return token_list\n",
    "\n",
    "with open('hayasugiru_maiso.txt', 'r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "lines = textdata.split(\"\\n\")#改行で分ける\n",
    "r = []\n",
    "n=0\n",
    "for line in lines:\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "    s = re.sub('\\u3000', '',s)\n",
    "    \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    #print(s,n)\n",
    "    n+=1\n",
    "    # Mecab\n",
    "    r.append(s)\n",
    "\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "matrix = vectorizer.fit_transform(r)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#いろいろな文で使える物にしたいと考えていたがテキストの書き方が統一されていないため\n",
    "#(at_a_railway_station　loaves　chichi_kaeruなど)\n",
    "#妥協したものとなっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('hayasugiru_maiso.txt', 'r')as f:\n",
    "    textdata=f.read()#一括で詠み込む インデント\n",
    "textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "#{m,n}\tm〜n回の繰り返し例\ta{2, 4}\taa, aaa, aaaa　\n",
    "#詠み込んだ文章を---部分で分割しタイトルと本文のみを取得\n",
    "#ここでは-の5回以上の繰り返しで分割している\n",
    "\n",
    "textdata = re.split('底本', textdata)[0]\n",
    "#詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "textdata = re.split('［＃ここから', textdata)[0]\n",
    "#詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "r = []\n",
    "for line in lines:\n",
    "\n",
    "    # 学習に使わない表現の削除処理\n",
    "    s = line\n",
    "    s = re.sub('｜', '',s)\n",
    "   \n",
    "    s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "    s = re.sub('［.+?］', '', s)#[]\n",
    "    s = re.sub('（.+?）', '', s)#()\n",
    "    # Mecab\n",
    "    \n",
    "    node = mecab.parseToNode(s)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "    while node:#ループにして情報を取得していく\n",
    "        # 単語を取得\n",
    "        word = node.surface#surfaceで表層形\n",
    "        # 品詞を取得\n",
    "        part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "        if part in [\"名詞\"]:#取得した品詞が一致するなら\n",
    "             r.append(word)#リストに追加   \n",
    "        node = node.next\n",
    "    \n",
    "c = collections.Counter(r)#語の出現個数をカウント、今回は\"名詞\", \"形容詞\", \"動詞\"をカウント\n",
    "print(\"各語の出現回数\",c.most_common(20))#表示 most_common()メソッドに引数nを指定すると、出現回数の多いn要素のみを返す。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #s = s.replace('\\u3000', '')#空白が半角ではなく全角の時使用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
