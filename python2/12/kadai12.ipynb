{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmv\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import collections\n",
    "import sys\n",
    "import MeCab\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#元の文書から名詞限定の文書を用意する\n",
    "mecab = MeCab.Tagger()\n",
    "def tokenize(text):\n",
    "    with open(text, 'r')as f:\n",
    "        textdata=f.read()#一括で詠み込む  \n",
    "        textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "        textdata = re.split('底本', textdata)[0]\n",
    "        #詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "        textdata = re.split('［＃ここから', textdata)[0]\n",
    "        #詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "        lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "        \n",
    "        results = []\n",
    "        for line in lines:\n",
    "            # 学習に使わない表現の削除処理\n",
    "            s = line\n",
    "            s = re.sub('｜', '',s)\n",
    "            s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "            s = re.sub('［.+?］', '', s)#[]\n",
    "            s = re.sub('（.+?）', '', s)#()\n",
    "\n",
    "            token_list=[]\n",
    "            node = mecab.parseToNode(s)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "            while node:#ループにして情報を取得していく\n",
    "                # 語を取得\n",
    "                word = node.surface#surfaceで表層形\n",
    "                # 品詞を取得\n",
    "                part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "                if part in [\"名詞\"]:#取得した品詞が一致するなら\n",
    "                     token_list.append(word)#リストに追加   \n",
    "                node = node.next\n",
    "            rl = (\" \".join(token_list)).strip()\n",
    "            results.append(rl)\n",
    "        w_file = \"result_\"+text\n",
    "        with open(w_file, 'w', encoding='utf-8') as wf:\n",
    "            wf.write(\"\\n\".join(results))  \n",
    "\n",
    "#小泉八雲\n",
    "tokenize('yukionna.txt') \n",
    "tokenize('noctilucae.txt') \n",
    "tokenize('hashino_uede.txt') \n",
    "tokenize('a_dead_secret.txt') \n",
    "tokenize('at_a_railway_station.txt') \n",
    "#夏目漱石による小説を5つ程度用意\n",
    "tokenize('ichiya.txt') \n",
    "tokenize('yukionna.txt') \n",
    "tokenize('carlyle_hakubutsukan.txt') \n",
    "tokenize('gubijinso.txt') \n",
    "tokenize('ganjitsu.txt') \n",
    "#\t太宰 治\n",
    "tokenize('umi.txt')\n",
    "tokenize('itto.txt')\n",
    "tokenize('ichimon_itto.txt')\n",
    "tokenize('aru_chukoku.txt')\n",
    "tokenize('anitachi.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize('umi.txt')\n",
    "tokenize('itto.txt')\n",
    "tokenize('ichimon_itto.txt')\n",
    "tokenize('aru_chukoku.txt')\n",
    "tokenize('anitachi.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.    0.   ...,  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...,  0.    0.    0.13]\n",
      " [ 0.    0.05  0.   ...,  0.    0.    0.  ]\n",
      " ..., \n",
      " [ 0.04  0.    0.   ...,  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...,  0.09  0.    0.  ]\n",
      " [ 0.    0.    0.   ...,  0.    0.    0.  ]]\n",
      "[['彼女' '巳之吉' '茂作' 'あなた' '小屋' '綺麗' '子供' 'よう' '老人' '渡し守' 'それ' 'お前' '江戸' '吹雪'\n",
      "  '彼等' 'わし' '約束' '少年' '少女' '自分']\n",
      " ['生命' '思惟' '夜光虫' '恒星' '変化' 'よう' '無限' '存在' '天の河' '霊的' '輝き' 'あまた' '水平' '波動'\n",
      "  '深紅' '小浪' '翠玉' '明滅' '鼓動' '閃光']\n",
      " ['わし' 'たち' '平七' '士官' '欄干' '熊本' 'らい' '薩摩' 'よう' '遠く' 'とき' '農家' '鎮台' '薩軍'\n",
      "  '草鞋' '百姓' '車夫' '灰色' '大砲' '辺り']\n",
      " ['カーライル' '婆さん' '倫敦' 'チェルシー' '演説' 'もの' '案内' '細君' 'よう' 'そう' 'これ' '四角' '夫人'\n",
      "  'チェイン' '往来' '御前' '村夫子' 'わし' 'ほか' 'ここ']\n",
      " ['甲野' '反吐' '雅号' 'さん' 'もの' '叡山' '四角' 'どこ' '静か' '馬鹿' '余計' '哲学' '愛嬌' 'ハハハハ'\n",
      "  'おれ' 'よう' 'だい' '大原女' 'いや' '落第']\n",
      " ['元日' '想像' '告白' '新聞' 'もの' '年始' '一昨年' '何処' '具眼' '上手' '正月' '虚子' '屑屋' '世間'\n",
      "  '去年' '困難' '調子' '読者' 'それ' '紙上']\n",
      " ['爆弾' '甲府' 'たち' '三鷹' '子供' 'ひとり' 'そこ' '五能線' '愕然' '東能代' '土地' '津軽' '貴重' '頭上'\n",
      "  '戦い' '浦島' 'まんなか' '全焼' '太郎' '津軽平野']\n",
      " ['バンザイ' 'たち' '紳士' '自動車' 'おでん' '行列' 'そう' '誕生' '卒業' 'いま' 'これ' '日本' 'もの'\n",
      "  'モオニング' '花火' '国旗' '歓喜' '奉祝' '機嫌' 'まわり']\n",
      " ['正直' 'よう' '最近' '感想' '生活' 'もの' '失敗' '展覧' '服装' 'キリスト' '主義' '場合' 'これ' '人間'\n",
      "  'アルバム' '現実' '理想' '聖書' '簡単' 'めった']\n",
      " ['和尚' '抽斗' '箪笥' '部屋' '貴女' '貼り紙' '心配' 'それ' 'もの' 'つぎ' '手紙' '住職' '飾り' '教育'\n",
      "  '商人' '道具' '秘密' '一同' 'よう' 'ため']\n",
      " ['彼女' '巳之吉' '茂作' 'あなた' '小屋' '綺麗' '子供' 'よう' '老人' '渡し守' 'それ' 'お前' '江戸' '吹雪'\n",
      "  '彼等' 'わし' '約束' '少年' '少女' '自分']\n",
      " ['生活' '責任' '此頃' '作品' 'お前' '明日' '扱い' 'まとも' '名士' 'お金' '覚悟' 'そう' '先生' 'きょう'\n",
      "  '小説' 'たち' 'それ' '戦地' 'いくら' 'パッション']\n",
      " ['犯人' '警部' '子ども' '坊や' 'たち' '熊本' '警察' 'こと' '警官' 'あんた' '巡査' 'とき' '兇徒' '改札'\n",
      "  '福岡' '殺人' '犯罪' '護送' '未亡人' 'よう']\n",
      " ['蜘蛛' '脚気' '蚊遣' '多く' '一疋' 'ククー' '彼ら' '三つ' '菓子' '合奏' '葛餅' '廻廊' '床柱' '膝頭'\n",
      "  'くし' '美人' 'たび' '一刻' '座敷' '羽団扇']\n",
      " ['長兄' 'こと' 'とき' '次兄' '雑誌' '愚僧' '発表' '喫茶店' 'よう' 'めし' 'それ' 'いま' 'さん' '兄たち'\n",
      "  '趣味' '三男' 'もの' '編輯' '川端' 'たち']]\n"
     ]
    }
   ],
   "source": [
    "# tf-idf を重みとする単語文書行列に変換\n",
    "#resultしたものは名詞限定になっている\n",
    "t1='result_yukionna.txt'\n",
    "t2='result_noctilucae.txt'\n",
    "t3='result_hashino_uede.txt'\n",
    "t4='result_carlyle_hakubutsukan.txt'\n",
    "t5='result_gubijinso.txt'\n",
    "t6='result_ganjitsu.txt'\n",
    "t7='result_umi.txt'\n",
    "t8='result_itto.txt'\n",
    "t9='result_ichimon_itto.txt'\n",
    "t10='result_a_dead_secret.txt'\n",
    "t11='result_yukionna.txt'\n",
    "t12='result_aru_chukoku.txt'\n",
    "t13='result_at_a_railway_station.txt'\n",
    "t14='result_ichiya.txt'\n",
    "t15='result_anitachi.txt'\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename')\n",
    "corpus=[t1,t2,t3,t4,t5,t6,t7,t8,t9,t10,t11,t12,t13,t14,t15]  \n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "#numpy の配列 ndarray のソーティングより\n",
    "\n",
    "#vectorizer.get_feature_names() 特徴名（単語）\n",
    "#tfidf_matrix.toarray()  # tf-idfの行列\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "print(population)\n",
    "indices = np.argsort(-population)\n",
    "#品詞を名詞に限定した場合\n",
    "#大きい値ほど各文書を特徴付ける単語　 値の多い順に20件表示\n",
    "print(name_list[indices[::,0:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### データのスケーリング（正規化）はtf-idfなのでOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['result_yukionna.txt' 'result_noctilucae.txt' 'result_hashino_uede.txt'\n",
      " 'result_carlyle_hakubutsukan.txt' 'result_gubijinso.txt'\n",
      " 'result_ganjitsu.txt' 'result_umi.txt' 'result_itto.txt'\n",
      " 'result_ichimon_itto.txt' 'result_a_dead_secret.txt' 'result_yukionna.txt'\n",
      " 'result_aru_chukoku.txt' 'result_at_a_railway_station.txt'\n",
      " 'result_ichiya.txt' 'result_anitachi.txt']\n",
      "[0 0 0 1 1 1 2 2 2 0 1 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "#文書の順番に作者番号が並んだ作者番号リスト（正解ラベルの1次元配列）を作成\n",
    "doc_info = pd.read_csv('token_files2.csv',encoding='cp932')\n",
    "#print(doc_info)\n",
    "files = doc_info['文書'].values\n",
    "print(files)\n",
    "author_codes = doc_info['著者コード'].values \n",
    "print(author_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#文書ベクトルと作者番号リストを，訓練用とテスト用に分割\n",
    "#今回は半分に\n",
    "docs_train    = population[:10]        # 訓練データ\n",
    "authors_train = author_codes[:10]  # 正解ラベル\n",
    "docs_test     = population[10:]         # テストデータ\n",
    "authors_test  = author_codes[10:]   # 正解ラベル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分類器を生成\n",
    "clf =SVC(kernel='linear',  C=1)#パラメータ設定は \n",
    "#sklearn.model_selection.GridSearchCV を使った\n",
    "# 訓練データを使って学習\n",
    "clf.fit(docs_train, authors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分類結果: [0 0 0 1 1 1 2 2 2 0]\n",
      "トレーニングデータに対する正解率：1.00\n"
     ]
    }
   ],
   "source": [
    "# トレーニングデータを分類\n",
    "pred_train = clf.predict(docs_train)\n",
    "print('分類結果:', pred_train)\n",
    "# 正解率 (accuracy) \n",
    "accuracy_train = accuracy_score(authors_train, pred_train)\n",
    "print('トレーニングデータに対する正解率：{0:.2f}'.format(accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分類結果: [0 0 0 0 0]\n",
      "テストデータに対する正解率：0.20\n"
     ]
    }
   ],
   "source": [
    "# テストデータに対する精度\n",
    "pred_test = clf.predict(docs_test)\n",
    "print('分類結果:', pred_test)\n",
    "accuracy_test = accuracy_score(authors_test, pred_test)\n",
    "print('テストデータに対する正解率：{0:.2f}'.format(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "混同行列：\n",
      "[[2 0]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "conf = confusion_matrix(authors_test, pred_test)\n",
    "print('混同行列：')\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "混同行列：\n",
      "[[3 0]\n",
      " [0 3]]\n"
     ]
    }
   ],
   "source": [
    "conf = confusion_matrix(authors_train, pred_train)\n",
    "print('混同行列：')\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "適合率，再現率，F値：\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         3\n",
      "          1       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       1.00      1.00      1.00         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_true=authors_train, y_pred=pred_train)\n",
    "print('適合率，再現率，F値：')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "適合率，再現率，F値：\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      1.00      0.80         2\n",
      "          1       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.83      0.75      0.73         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_true=authors_test, y_pred=pred_test)\n",
    "print('適合率，再現率，F値：')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Standard = svm.LinearSVC(penalty='l2', loss='hinge', dual=True, tol=1e-3)\n",
    "LossL2 = svm.LinearSVC(\n",
    "        penalty='l1', loss='squared_hinge', dual=False, tol=1e-3)\n",
    "PenaltyL1 = svm.LinearSVC(\n",
    "        penalty='l2', loss='squared_hinge', dual=True, tol=1e-3)\n",
    "model_set = [Standard, LossL2, PenaltyL1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tuned_parameters = [\n",
    "        {'C': [1, 10, 100, 1000], 'kernel': ['linear']}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = 'f1'\n",
    "clf = GridSearchCV(\n",
    "    SVC(), # 識別器\n",
    "    tuned_parameters, # 最適化したいパラメータセット \n",
    "    cv=2, # 交差検定の回数\n",
    "    scoring='%s_weighted' % score ) # モデルの評価関数の指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmv\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\fmv\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\fmv\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\fmv\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='f1_weighted', verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(docs_train, authors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 各モデルを一つずつCに関してグリッドサーチを行う\n",
    "for model in model_set:\n",
    "    # 5-fold Cross-Validationで評価指標はprecision(適合率)を選択  \n",
    "    clf = GridSearchCV(model, tuned_parameters, cv=2, scoring='precision')\n",
    "    # 設定したパラメータで学習しつつ検証を行う\n",
    "    clf.fit(docs_train, authors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1='result_yukionna.txt'\n",
    "t2='result_noctilucae.txt'\n",
    "t3='result_hashino_uede.txt'\n",
    "t4='result_a_dead_secret.txt'\n",
    "t5='result_at_a_railway_station.txt'\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename')\n",
    "corpus=[t1,t2,t3,t4,t5]  \n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "\n",
    "#numpy の配列 ndarray のソーティングより\n",
    "\n",
    "#vectorizer.get_feature_names() 特徴名（単語）\n",
    "#tfidf_matrix.toarray()  # tf-idfの行列\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "print(population)\n",
    "indices = np.argsort(-population)\n",
    "#品詞を名詞に限定した場合\n",
    "#大きい値ほど各文書を特徴付ける単語　 値の多い順に20件表示\n",
    "print(name_list[indices[::,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = svm.SVC(kernel='rbf', C=1, gamma=0.1)\n",
    "#svm.SVC()#SVC(kernel='linear', random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
