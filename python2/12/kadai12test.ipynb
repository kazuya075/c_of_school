{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fmv\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import collections\n",
    "import sys\n",
    "import MeCab\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#名詞限定\n",
    "mecab = MeCab.Tagger()\n",
    "def tokenize(text):\n",
    "    with open(text, 'r')as f:\n",
    "        textdata=f.read()#一括で詠み込む  \n",
    "        textdata = re.split('-{5,}', textdata)[0]+ re.split('-{5,}', textdata)[2]\n",
    "        textdata = re.split('底本', textdata)[0]\n",
    "        #詠み込んだ文章を底本部分で分割し本文のみを取得\n",
    "        textdata = re.split('［＃ここから', textdata)[0]\n",
    "        #詠み込んだ文章を［＃ここから　部分で分割し本文のみを取得\n",
    "        lines = textdata.split(\"\\r\\n\")#改行で分ける\n",
    "        \n",
    "        results = []\n",
    "        for line in lines:\n",
    "            # 学習に使わない表現の削除処理\n",
    "            s = line\n",
    "            s = re.sub('｜', '',s)\n",
    "            s = re.sub('《.+?》', '', s)# 《》を消す\n",
    "            s = re.sub('［.+?］', '', s)#[]\n",
    "            s = re.sub('（.+?）', '', s)#()\n",
    "\n",
    "            token_list=[]\n",
    "            node = mecab.parseToNode(s)#parseToNode()を使うと形態素の詳細情報\n",
    "    \n",
    "            while node:#ループにして情報を取得していく\n",
    "                # 語を取得\n",
    "                word = node.surface#surfaceで表層形\n",
    "                # 品詞を取得\n",
    "                part = node.feature.split(\",\")[0]#featureで形態素情報　[0]で品詞取得\n",
    "\n",
    "                if part in [\"名詞\"]:#取得した品詞が一致するなら\n",
    "                     token_list.append(word)#リストに追加   \n",
    "                node = node.next\n",
    "            rl = (\" \".join(token_list)).strip()\n",
    "            results.append(rl)\n",
    "        w_file = \"result_\"+text\n",
    "        with open(w_file, 'w', encoding='utf-8') as wf:\n",
    "            wf.write(\"\\n\".join(results))  \n",
    "\n",
    "\n",
    "tokenize('yukionna.txt') \n",
    "tokenize('noctilucae.txt') \n",
    "tokenize('hashino_uede.txt') \n",
    "tokenize('a_dead_secret.txt') \n",
    "tokenize('at_a_railway_station.txt') \n",
    "\n",
    "tokenize('ichiya.txt') \n",
    "tokenize('yukionna.txt') \n",
    "tokenize('carlyle_hakubutsukan.txt') \n",
    "tokenize('gubijinso.txt') \n",
    "tokenize('ganjitsu.txt') \n",
    "#, \"ignore\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.    0.   ...,  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...,  0.06  0.12  0.12]\n",
      " [ 0.    0.04  0.   ...,  0.    0.    0.  ]\n",
      " [ 0.    0.    0.04 ...,  0.    0.    0.  ]\n",
      " [ 0.04  0.    0.   ...,  0.    0.    0.  ]]\n",
      "[['彼女' '巳之吉' 'あなた' '茂作' '小屋' 'よう' '綺麗' '子供' 'それ' '老人' '渡し守' 'わし' 'お前' '彼等'\n",
      "  '吹雪' '少女' 'あと' '江戸' '自分' '約束']\n",
      " ['生命' 'よう' '夜光虫' '思惟' '無限' '変化' '存在' '恒星' 'もの' '波動' '輝き' '深紅' '一瞬' '天の河'\n",
      "  '小浪' '翠玉' '水平' '閃光' '黄色' '鼓動']\n",
      " ['たち' 'わし' '平七' '士官' '欄干' 'よう' '熊本' 'とき' '薩摩' '遠く' 'らい' 'そこ' 'ここ' 'ところ'\n",
      "  '農家' 'みんな' '薩軍' '辺り' '灰色' '鎮台']\n",
      " ['和尚' '抽斗' '箪笥' '部屋' '貴女' 'もの' 'それ' '手紙' '心配' '貼り紙' 'よう' 'つぎ' '飾り' '修業'\n",
      "  '住職' '秘密' '一同' '道具' '教育' '一番']\n",
      " ['犯人' 'たち' 'こと' '子ども' '警部' '坊や' '警察' '熊本' 'とき' 'よう' 'あんた' '警官' '巡査' 'ここ'\n",
      "  '未亡人' '改札' '殺人' '犯罪' '護送' '兇徒']]\n"
     ]
    }
   ],
   "source": [
    "t1='result_yukionna.txt'\n",
    "t2='result_noctilucae.txt'\n",
    "t3='result_hashino_uede.txt'\n",
    "t4='result_a_dead_secret.txt'\n",
    "t5='result_at_a_railway_station.txt'\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename')\n",
    "corpus=[t1,t2,t3,t4,t5]  \n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "\n",
    "#numpy の配列 ndarray のソーティングより\n",
    "\n",
    "#vectorizer.get_feature_names() 特徴名（単語）\n",
    "#tfidf_matrix.toarray()  # tf-idfの行列\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "print(population)\n",
    "indices = np.argsort(-population)\n",
    "#品詞を名詞に限定した場合\n",
    "#大きい値ほど各文書を特徴付ける単語　 値の多い順に20件表示\n",
    "print(name_list[indices[::,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['蜘蛛' '脚気' '蚊遣' '多く' '一疋' '三つ' 'ククー' '彼ら' '菓子' '葛餅' '廻廊' '膝頭' '美人' 'くし'\n",
      "  '床柱' '合奏' 'もの' 'あたり' '一刻' '座敷']\n",
      " ['彼女' '巳之吉' '茂作' 'あなた' '小屋' '子供' 'よう' '綺麗' '老人' 'それ' '渡し守' 'お前' '少年' '彼等'\n",
      "  '約束' '少女' '江戸' '吹雪' '自分' 'わし']\n",
      " ['カーライル' '婆さん' '倫敦' 'もの' 'チェルシー' 'よう' '演説' 'そう' '案内' 'これ' '細君' '四角' '部屋'\n",
      "  'わし' 'ここ' '往来' '村夫子' '御前' 'チェイン' '夫人']\n",
      " ['甲野' 'さん' 'もの' '反吐' '雅号' '叡山' 'どこ' 'よう' '静か' 'おれ' '四角' 'だい' 'ハハハハ' '愛嬌'\n",
      "  '哲学' '余計' '馬鹿' 'いや' '大原女' '袖無']\n",
      " ['元日' '新聞' '告白' '想像' 'もの' '一昨年' '具眼' '年始' '何処' '屑屋' '上手' '虚子' '困難' '読者'\n",
      "  '正月' '去年' '世間' 'それ' '調子' '材料']]\n"
     ]
    }
   ],
   "source": [
    "t1='result_ichiya.txt'\n",
    "t2='result_yukionna.txt'\n",
    "t3='result_carlyle_hakubutsukan.txt'\n",
    "t4='result_gubijinso.txt'\n",
    "t5='result_ganjitsu.txt'\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename')\n",
    "corpus=[t1,t2,t3,t4,t5]  \n",
    "matrix = vectorizer.fit_transform(corpus)\n",
    "#CountVectorizer で得られた単語文書行列を，TfidfTransformer でtf-idfの行列に変換する\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_matrix = tfidf.fit_transform(matrix)\n",
    "np.set_printoptions(precision=2) \n",
    "\n",
    "#numpy の配列 ndarray のソーティングより\n",
    "\n",
    "#vectorizer.get_feature_names() 特徴名（単語）\n",
    "#tfidf_matrix.toarray()  # tf-idfの行列\n",
    "name_list = np.array(vectorizer.get_feature_names())\n",
    "population = np.array(tfidf_matrix.toarray())\n",
    "#print(population)\n",
    "indices = np.argsort(-population)\n",
    "#品詞を名詞に限定した場合\n",
    "#大きい値ほど各文書を特徴付ける単語　 値の多い順に20件表示\n",
    "print(name_list[indices[::,0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "doc_info = pd.read_csv('token_files.csv',encoding='cp932')\n",
    "print(doc_info)\n",
    "files = doc_info['文書'].values\n",
    "print(files)\n",
    "author_codes = doc_info['著者コード'].values \n",
    "print(author_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_train    = population[:6]        # 訓練データ\n",
    "authors_train = author_codes[:6]  # 正解ラベル\n",
    "docs_test     = population[6:]         # テストデータ\n",
    "authors_test  = author_codes[6:]   # 正解ラベル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-74297ae1248c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthor_codes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\fmv\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2037\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2038\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2039\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2040\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstratify\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2041\u001b[0m         cv = StratifiedShuffleSplit(stratify, test_size=test_size,\n",
      "\u001b[1;32mC:\\Users\\fmv\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\fmv\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 181\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5, 10]"
     ]
    }
   ],
   "source": [
    "X =np.array(population)\n",
    "y =np.array(author_codes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "kf = KFold(5, n_folds=3)\n",
    "for train, test in kf:\n",
    "    X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "    clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear', random_state=None)\n",
    "clf.fit(docs_train ,authors_train)\n",
    "output = clf.predict(docs_test)\n",
    "print('分類結果:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_info.to_csv('token_files.csv', columns=['文書'])\n",
    "#著者コード\n",
    "\n",
    "#t1='result_akai_heya.txt'\n",
    "#t2='result_carmen.txt'\n",
    "#t3='result_hayasugiru_maiso.txt'\n",
    "#t4='result_meijinden.txt'\n",
    "#t1='result_akai_heya.txt'\n",
    "#t2='result_carmen.txt'\n",
    "#t3='result_hayasugiru_maiso.txt'\n",
    "#t4='result_meijinden.txt'\n",
    "\n",
    "\n",
    "#t2='result_carmen.txt'\n",
    "\n",
    "with open(r'C:\\Users\\fmv\\Desktop\\python\\12\\result_carmen.txt', 'r',encoding=\"utf-8_sig\")as f:\n",
    "    t2=f.read()#一括で詠み込む\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
